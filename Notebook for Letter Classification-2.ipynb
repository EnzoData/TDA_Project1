{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages needed for the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed initially\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "from scipy import ndimage\n",
    "import PIL\n",
    "from persim import plot_diagrams\n",
    "from ripser import ripser, lower_star_img \n",
    "import csv\n",
    "from numpy import genfromtxt \n",
    "import persim as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Letters File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = genfromtxt('letters.csv', delimiter=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Next Steps are to Perform Filtrations and Convert those Persistent Diagrams to a Meaningful Value: Our first example will be left to right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we Perform the Filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 2., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 4.,  5.],\n",
       "        [ 4., inf]]),\n",
       " array([[ 4.,  6.],\n",
       "        [ 4., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 6.,  7.],\n",
       "        [ 2., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 3.,  8.],\n",
       "        [ 3., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 2., inf]]),\n",
       " array([[ 3.,  4.],\n",
       "        [ 3., inf]]),\n",
       " array([[ 3., inf]]),\n",
       " array([[ 3.,  7.],\n",
       "        [ 2., inf]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Left to Right Scanning and Conversion\n",
    "dgmLR = [None]*26 #Initialize an empty list\n",
    "for i in range(26):\n",
    "    letter_one_line=letters[i,:]\n",
    "\n",
    "    # initialize matrix of size 10x10 with all values 100\n",
    "    letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "    for k in range(1,101):\n",
    "        if letter_one_line[k]==1.0:\n",
    "            row=int((k-1)/10) \n",
    "            column=(k-1)%10 \n",
    "            letter[row,column]=k%10            \n",
    "    dgmLR[i] = lower_star_img(letter)\n",
    "dgmLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take this list of persistent diagrams and find the pairwise bottleneck distance between letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [2. , 1. , 2. , 1. , 1. , 1. , 2. , 1. , 0. , 1. , 1. , 1. , 2. ,\n",
       "        2. , 2. , 1. , 2. , 1. , 2.5, 2. , 1. , 2. , 2. , 1. , 1. , 2. ],\n",
       "       [2. , 1. , 2. , 1. , 1. , 1. , 2. , 1. , 1. , 0. , 1. , 1. , 2. ,\n",
       "        2. , 2. , 1. , 2. , 1. , 2. , 2. , 1. , 2. , 2. , 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [0.5, 1. , 0.5, 1. , 1. , 1. , 0.5, 1. , 2. , 2. , 1. , 1. , 0.5,\n",
       "        0.5, 0.5, 1. , 0. , 1. , 2.5, 0.5, 1. , 0.5, 0.5, 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2. , 2.5, 2.5, 2.5,\n",
       "        2.5, 2.5, 2.5, 2.5, 2.5, 0. , 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 1. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [0. , 1. , 0. , 1. , 1. , 1. , 0. , 1. , 2. , 2. , 1. , 1. , 0. ,\n",
       "        0. , 0. , 1. , 0.5, 1. , 2.5, 0. , 1. , 0. , 0. , 1. , 1. , 2. ],\n",
       "       [1. , 0.5, 1. , 0.5, 0.5, 0.5, 1. , 0.5, 1. , 1. , 0.5, 0.5, 1. ,\n",
       "        1. , 1. , 0.5, 1. , 0.5, 2.5, 1. , 0.5, 1. , 1. , 0. , 0.5, 2. ],\n",
       "       [1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 1. , 0. , 0. , 1. ,\n",
       "        1. , 1. , 0. , 1. , 0. , 2.5, 1. , 0. , 1. , 1. , 0.5, 0. , 2. ],\n",
       "       [2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. ,\n",
       "        2. , 2. , 2. , 2. , 2. , 1. , 2. , 2. , 2. , 2. , 2. , 2. , 0. ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set an empty pairwise distance matrix for future bottleneck distance input\n",
    "BNDLR = np.zeros((26,26))\n",
    "\n",
    "\n",
    "# Change infinities to very large numbers\n",
    "for i in range(26):\n",
    "    dgmLR[i][np.isinf(dgmLR[i])] = 10000\n",
    "\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "for i in range(26):\n",
    "    for j in range(26):\n",
    "        BNDLR[i,j] = pm.bottleneck(dgmLR[i], dgmLR[j])\n",
    "        \n",
    "# The very large values should be set to 0 by bottleneck definity (since the very large distances would be inifinity)\n",
    "BNDLR[BNDLR>1000]=0\n",
    "BNDLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run an agglomorative clustering method on the pairwise distance to grab values for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 1, 1, 2, 1, 0, 0, 1, 1, 2, 2, 2, 1, 2, 1, 3, 2, 1, 2,\n",
       "       2, 1, 1, 4], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we look to perform clustering on this pairwise distance matrix. For simplicity we use sklearn's Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# For now we set 5 clusters, but we can change this and look for better results in our model. A lower number here will prevent overfitting.\n",
    "clusteringLR = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(BNDLR)\n",
    "\n",
    "# This will output a vector of length 26 representing a number for each letter for this scan.\n",
    "LR_test = clusteringLR.labels_\n",
    "LR_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have an array of size 26, where each value corresponds to that letter's value. I will now perform this methodology for the other scannings. (Feel free to skip the reading of this code, it is the same as above's code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Putts\\Anaconda3\\lib\\site-packages\\ripser\\ripser.py:342: RuntimeWarning: invalid value encountered in maximum\n",
      "  thisD = np.maximum(thisD, tD)\n",
      "C:\\Users\\Putts\\Anaconda3\\lib\\site-packages\\ripser\\ripser.py:342: RuntimeWarning: invalid value encountered in maximum\n",
      "  thisD = np.maximum(thisD, tD)\n",
      "C:\\Users\\Putts\\Anaconda3\\lib\\site-packages\\ripser\\ripser.py:342: RuntimeWarning: invalid value encountered in maximum\n",
      "  thisD = np.maximum(thisD, tD)\n"
     ]
    }
   ],
   "source": [
    "# Right to Left Scanning\n",
    "\n",
    "dgmRL = [None]*26 #Initialize an empty list\n",
    "for i in range(26):\n",
    "    letter_one_line=letters[i,:]\n",
    "\n",
    "    # initialize matrix of size 10x10 with all values 100\n",
    "    letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "    for k in range(1,101):\n",
    "        if letter_one_line[k]==1.0:\n",
    "            row=int((k-1)/10) \n",
    "            column=(k-1)%10 \n",
    "            letter[row,column]=10-k%10            \n",
    "    dgmRL[i] = lower_star_img(letter)\n",
    "\n",
    "# Set an empty pairwise distance matrix for future bottleneck distance input\n",
    "BNDRL = np.zeros((26,26))\n",
    "\n",
    "\n",
    "# Change infinities to very large numbers\n",
    "for i in range(26):\n",
    "    dgmRL[i][np.isinf(dgmRL[i])] = 10000\n",
    "\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "for i in range(26):\n",
    "    for j in range(26):\n",
    "        BNDRL[i,j] = pm.bottleneck(dgmRL[i], dgmRL[j])\n",
    "\n",
    "# The very large values should be set to 0 by bottleneck definity (since the very large distances would be inifinity)\n",
    "BNDRL[BNDRL>1000]=0\n",
    "\n",
    "# Now we look to perform clustering on this pairwise distance matrix. For simplicity we use sklearn's Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# For now we set 3 clusters, but we can change this and look for better results in our model. A lower number here will prevent overfitting.\n",
    "clusteringRL = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(BNDRL)\n",
    "\n",
    "# This will output a vector of length 26 representing a number for each letter for this scan.\n",
    "RL_test = clusteringRL.labels_\n",
    "\n",
    "# Angle Scanning\n",
    "\n",
    "dgmAngle = [None]*26 #Initialize an empty list\n",
    "for i in range(26):\n",
    "    letter_one_line=letters[i,:]\n",
    "\n",
    "    # initialize matrix of size 10x10 with all values 100\n",
    "    letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "    for k in range(1,101):\n",
    "        if letter_one_line[k]==1.0:\n",
    "            row=int((k-1)/10) \n",
    "            column=(k-1)%10 \n",
    "            letter[row,column]=max(k%10,int(k-1)%10)            \n",
    "    dgmAngle[i] = lower_star_img(letter)\n",
    "\n",
    "\n",
    "# Set an empty pairwise distance matrix for future bottleneck distance input\n",
    "BNDAngle = np.zeros((26,26))\n",
    "\n",
    "\n",
    "# Change infinities to very large numbers\n",
    "for i in range(26):\n",
    "    dgmAngle[i][np.isinf(dgmAngle[i])] = 10000\n",
    "\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "for i in range(26):\n",
    "    for j in range(26):\n",
    "        BNDAngle[i,j] = pm.bottleneck(dgmAngle[i], dgmAngle[j])\n",
    "\n",
    "# The very large values should be set to 0 by bottleneck definity (since the very large distances would be inifinity)\n",
    "BNDAngle[BNDAngle>1000]=0\n",
    "\n",
    "# Now we look to perform clustering on this pairwise distance matrix. For simplicity we use sklearn's Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# For now we set 3 clusters, but we can change this and look for better results in our model. A lower number here will prevent overfitting.\n",
    "clusteringAngle = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(BNDAngle)\n",
    "\n",
    "# This will output a vector of length 26 representing a number for each letter for this scan.\n",
    "A_test = clusteringAngle.labels_\n",
    "\n",
    "# Diagonal Scanning\n",
    "    \n",
    "dgmDiagonal = [None]*26 #Initialize an empty list\n",
    "for i in range(26):\n",
    "    letter_one_line=letters[i,:]\n",
    "\n",
    "    # initialize matrix of size 10x10 with all values 100\n",
    "    letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "    for k in range(1,101):\n",
    "        if letter_one_line[k]==1.0:\n",
    "            row=int((k-1)/10) \n",
    "            column=(k-1)%10 \n",
    "            letter[row,column]=(column+row)*k%10            \n",
    "    dgmDiagonal[i] = lower_star_img(letter)\n",
    "    \n",
    "# Set an empty pairwise distance matrix for future bottleneck distance input\n",
    "BNDDiagonal = np.zeros((26,26))\n",
    "\n",
    "\n",
    "# Change infinities to very large numbers\n",
    "for i in range(26):\n",
    "    dgmDiagonal[i][np.isinf(dgmDiagonal[i])] = 10000\n",
    "\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "for i in range(26):\n",
    "    for j in range(26):\n",
    "        BNDDiagonal[i,j] = pm.bottleneck(dgmDiagonal[i], dgmDiagonal[j])\n",
    "\n",
    "# The very large values should be set to 0 by bottleneck definity (since the very large distances would be inifinity)\n",
    "BNDDiagonal[BNDDiagonal>1000]=0\n",
    "\n",
    "# Now we look to perform clustering on this pairwise distance matrix. For simplicity we use sklearn's Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# For now we set 3 clusters, but we can change this and look for better results in our model. A lower number here will prevent overfitting.\n",
    "clusteringDiagonal = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(BNDDiagonal)\n",
    "\n",
    "# This will output a vector of length 26 representing a number for each letter for this scan.\n",
    "D_test = clusteringDiagonal.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about the runtime warning, it isn't an error in the code. The next step after this is to grab some variables that test the densities of portions of the grid. This helps differentiate R & B, O & Q, and some others. The areas we found the density of should be large enough not to overfit if some noise occurred in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test to differentiate some letters\n",
    "bottom_test = [None]*26\n",
    "for i in range(26):\n",
    "    bottom_test[i]=sum(letters[i][51:101])\n",
    "    \n",
    "right_test = [None]*26  \n",
    "for i in range(26):\n",
    "    right_test[i]=sum(np.concatenate((letters[i][6:11],\n",
    "              letters[i][16:21],\n",
    "              letters[i][26:31],\n",
    "              letters[i][36:41],\n",
    "              letters[i][46:51],\n",
    "              letters[i][56:61],\n",
    "              letters[i][66:71],\n",
    "              letters[i][76:81],\n",
    "              letters[i][86:91],\n",
    "              letters[i][96:101]\n",
    "              )))\n",
    "\n",
    "botright = [None]*26\n",
    "for i in range(26):\n",
    "    botright[i] = sum(np.concatenate((\n",
    "              letters[i][56:61],\n",
    "              letters[i][66:71],\n",
    "              letters[i][76:81],\n",
    "              letters[i][86:91],\n",
    "              letters[i][96:101]\n",
    "              )))\n",
    "\n",
    "top_test = [None]*26\n",
    "for i in range(26):\n",
    "    top_test[i] = sum(letters[i][1:51])\n",
    "    \n",
    "density_test = [None]*26\n",
    "for i in range(26):\n",
    "    density_test[i] = sum(letters[i][1:101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine all of our newly found variables into a neat matrix. This will be our new design matrix to input into our prediction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  2.,  2., 13., 13.,  6., 14., 27.],\n",
       "       [ 1.,  0.,  1.,  0., 13., 16.,  7., 20., 33.],\n",
       "       [ 2.,  2.,  2.,  0., 11.,  7.,  4., 10., 21.],\n",
       "       [ 1.,  0.,  1.,  0., 15., 16.,  9., 13., 28.],\n",
       "       [ 1.,  3.,  1.,  1.,  9.,  9.,  3., 14., 23.],\n",
       "       [ 1.,  2.,  1.,  0.,  4.,  4.,  0., 12., 16.],\n",
       "       [ 2.,  2.,  2.,  2., 13.,  7.,  5., 10., 23.],\n",
       "       [ 1.,  4.,  1.,  0.,  6.,  8.,  3., 11., 17.],\n",
       "       [ 0.,  1.,  0.,  2.,  6.,  2.,  1.,  6., 12.],\n",
       "       [ 0.,  1.,  0.,  0.,  5.,  7.,  3.,  6., 11.],\n",
       "       [ 1.,  2.,  1.,  0., 11.,  8.,  4., 11., 22.],\n",
       "       [ 1.,  4.,  1.,  0.,  8.,  2.,  2.,  4., 12.],\n",
       "       [ 2.,  0.,  2.,  0.,  9., 12.,  3., 19., 28.],\n",
       "       [ 2.,  4.,  2.,  2., 10., 10.,  6., 13., 23.],\n",
       "       [ 2.,  0.,  2.,  0., 13., 12.,  6., 12., 25.],\n",
       "       [ 1.,  0.,  1.,  0.,  8.,  9.,  1., 14., 22.],\n",
       "       [ 2.,  0.,  2.,  1., 15., 14.,  8., 13., 28.],\n",
       "       [ 1.,  1.,  1.,  0., 12., 10.,  4., 13., 25.],\n",
       "       [ 3.,  2.,  3.,  1., 13., 11.,  8., 10., 23.],\n",
       "       [ 2.,  0.,  2.,  4.,  4.,  3.,  0., 10., 14.],\n",
       "       [ 1.,  0.,  1.,  1., 12., 10.,  6.,  8., 20.],\n",
       "       [ 2.,  0.,  2.,  3.,  9.,  9.,  4., 11., 20.],\n",
       "       [ 2.,  0.,  2.,  0., 15., 17.,  8., 17., 32.],\n",
       "       [ 1.,  1.,  1.,  2., 12., 11.,  5., 13., 25.],\n",
       "       [ 1.,  4.,  1.,  0.,  7.,  6.,  1., 10., 17.],\n",
       "       [ 4.,  2.,  4.,  0., 11., 11.,  3., 13., 24.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.concatenate((LR_test,\n",
    "                             RL_test,\n",
    "                             A_test,\n",
    "                             D_test,\n",
    "                             bottom_test,\n",
    "                             right_test,\n",
    "                             botright,\n",
    "                             top_test,\n",
    "                             density_test))\n",
    "test_array = np.reshape(test_array,(9,26)).T\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that we would run it through the most simple algorithm for predicting multiple classes. Just a normal multinomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Putts\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Putts\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
       "       14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab our labels\n",
    "training_labels = [None]*26\n",
    "for i in range(26):\n",
    "    training_labels[i] = letters[i][0]\n",
    "\n",
    "\n",
    "# Fit our model and see if it has 100% accuracy on training data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LogReg=LogisticRegression()\n",
    "LogReg.fit(test_array, training_labels)\n",
    "y_pred = LogReg.predict(test_array)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is 100% accuracy for the original data. I now look to set up this notebook so that you can input new vectors to test. Please add some random number in the beginning. For example I started with a 1 since it was A. This is just there so the indexing works. You could put a string there if you wanted for reference. This example is a noisy A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLetter=[1,0., 0., 0., 0., 1., 0, 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
    "0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
    "0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
    "0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
    "    0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look to perform all filtrations on the new letter, as well as the density tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Putts\\Anaconda3\\lib\\site-packages\\ripser\\ripser.py:342: RuntimeWarning: invalid value encountered in maximum\n",
      "  thisD = np.maximum(thisD, tD)\n"
     ]
    }
   ],
   "source": [
    " #Left to Right Filtration\n",
    "letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "for k in range(1,101):\n",
    "    if newLetter[k]==1.0:\n",
    "        row=int((k-1)/10) \n",
    "        column=(k-1)%10 \n",
    "        letter[row,column]=k%10            \n",
    "dgmNLR = lower_star_img(letter)\n",
    "\n",
    " # Right to Left Filtration\n",
    "letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "for k in range(1,101):\n",
    "    if newLetter[k]==1.0:\n",
    "        row=int((k-1)/10) \n",
    "        column=(k-1)%10 \n",
    "        letter[row,column]=10-k%10            \n",
    "dgmNRL = lower_star_img(letter)\n",
    "    \n",
    " # Angle Filtration\n",
    "letter=np.full((10, 10), 100)\n",
    "\n",
    "# convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "for k in range(1,101):\n",
    "    if newLetter[k]==1.0:\n",
    "        row=int((k-1)/10) \n",
    "        column=(k-1)%10 \n",
    "        letter[row,column]=max(k%10,int(k-1)%10)            \n",
    "dgmNA = lower_star_img(letter)\n",
    "    \n",
    " # Diagonal Filtration\n",
    "letter=np.full((10, 10), 100)\n",
    "\n",
    "    # convert one line letter to 10x10 matrix replacing zeros with 100\n",
    "for k in range(1,101):\n",
    "    if newLetter[k]==1.0:\n",
    "        row=int((k-1)/10) \n",
    "        column=(k-1)%10 \n",
    "        letter[row,column]=(column+row)*k%10            \n",
    "dgmND = lower_star_img(letter)\n",
    "\n",
    "# A test to differentiate some letters\n",
    "\n",
    "bottom_test=sum(newLetter[51:101])\n",
    "    \n",
    "\n",
    "right_test=sum(np.concatenate((newLetter[6:11],\n",
    "              newLetter[16:21],\n",
    "              newLetter[26:31],\n",
    "              newLetter[36:41],\n",
    "              newLetter[46:51],\n",
    "              newLetter[56:61],\n",
    "              newLetter[66:71],\n",
    "              newLetter[76:81],\n",
    "              newLetter[86:91],\n",
    "              newLetter[96:101]\n",
    "              )))\n",
    "\n",
    "botright = sum(np.concatenate((\n",
    "              newLetter[56:61],\n",
    "              newLetter[66:71],\n",
    "              newLetter[76:81],\n",
    "              newLetter[86:91],\n",
    "              newLetter[96:101]\n",
    "              )))\n",
    "\n",
    "top_test = sum(newLetter[1:51])\n",
    "\n",
    "density_test = sum(newLetter[1:101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the filtrations to find the bottleneck distance between the new letter and all of the old ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change infinities to very large numbers\n",
    "dgmNLR[np.isinf(dgmNLR)] = 10000\n",
    "dgmNRL[np.isinf(dgmNRL)] = 10000\n",
    "dgmNA[np.isinf(dgmNA)] = 10000\n",
    "dgmND[np.isinf(dgmND)] = 10000\n",
    "\n",
    "# Find bottleneck distance between new letter and previous letters\n",
    "\n",
    "# Left to Right\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "BNDNLR = [None]*26\n",
    "for i in range(26):\n",
    "    BNDNLR[i] = pm.bottleneck(dgmLR[i],dgmNLR)\n",
    "BNDNLR = np.array(BNDNLR)\n",
    "BNDNLR[BNDNLR>1000]=0\n",
    "\n",
    "# Right to Left\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "BNDNRL = [None]*26\n",
    "for i in range(26):\n",
    "    BNDNRL[i] = pm.bottleneck(dgmRL[i],dgmNRL)\n",
    "BNDNRL = np.array(BNDNRL) \n",
    "BNDNRL[BNDNRL>1000]=0\n",
    "\n",
    "# Angle\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "BNDNA = [None]*26\n",
    "for i in range(26):\n",
    "    BNDNA[i] = pm.bottleneck(dgmAngle[i],dgmNA)\n",
    "BNDNA = np.array(BNDNA)\n",
    "BNDNA[BNDNA>1000]=0\n",
    "    \n",
    "# Diagonoal\n",
    "# Calculate bottleneck distances and input into the pairwise matrix\n",
    "BNDND = [None]*26\n",
    "for i in range(26):\n",
    "    BNDND[i] = pm.bottleneck(dgmDiagonal[i],dgmND)\n",
    "BNDND = np.array(BNDND)\n",
    "BNDND[BNDND>1000]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert them into their values using Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left to Right Value\n",
    "temp=np.vstack((BNDLR,BNDNLR))\n",
    "temp2=np.append(BNDNLR,0)\n",
    "NewBNDLR = np.hstack((temp, np.atleast_2d(temp2).T))\n",
    "LRClust = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(NewBNDLR)\n",
    "LRValue=LRClust.labels_[26]\n",
    "\n",
    "# Right to Left Value\n",
    "temp=np.vstack((BNDRL,BNDNRL))\n",
    "temp2=np.append(BNDNRL,0)\n",
    "NewBNDRL = np.hstack((temp, np.atleast_2d(temp2).T))\n",
    "RLClust = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(NewBNDRL)\n",
    "RLValue=RLClust.labels_[26]\n",
    "\n",
    "\n",
    "# Angle Value\n",
    "temp=np.vstack((BNDAngle,BNDNA))\n",
    "temp2=np.append(BNDNA,0)\n",
    "NewBNDA = np.hstack((temp, np.atleast_2d(temp2).T))\n",
    "AngleClust = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(NewBNDA)\n",
    "AValue=AngleClust.labels_[26]\n",
    "\n",
    "\n",
    "# Diagonal Value\n",
    "temp=np.vstack((BNDDiagonal,BNDND))\n",
    "temp2=np.append(BNDND,0)\n",
    "NewBNDD = np.hstack((temp, np.atleast_2d(temp2).T))\n",
    "DiagonalClust = AgglomerativeClustering(n_clusters = 5,\n",
    "                                     affinity = \"precomputed\",\n",
    "                                     linkage = \"average\").fit(NewBNDD)\n",
    "DValue=DiagonalClust.labels_[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine these values into one vector and run our model on that vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New_Letter = np.array((LRValue,\n",
    "                             RLValue,\n",
    "                             AValue,\n",
    "                             DValue,\n",
    "                             bottom_test,\n",
    "                             right_test,\n",
    "                             botright,\n",
    "                             top_test,\n",
    "                             density_test))\n",
    "LogReg.predict(New_Letter.reshape(1,-1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier isn't the best with noise. It would be a much better classifier if we included the regular persistent homology diagram up to dimension 1. Also, if we include an L_1 regularization term to our multinomial regression then that would also have helped."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
